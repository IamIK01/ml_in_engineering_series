{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2415533,"sourceType":"datasetVersion","datasetId":1461399}],"dockerImageVersionId":30822,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline \nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hlth_df = pd.read_csv(filepath)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hlth_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hlth_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"hlth_df.hist(bins=50, figsize=(20, 15))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\naxes = scatter_matrix(hlth_data, figsize=(32, 16))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nsns.heatmap(hlth_df.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Transformer Health Metrics')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nsns.heatmap(hlth_df.corr(\"kendall\"), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Transformer Health Metrics')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(14, 10))\nsns.heatmap(hlth_df.corr(\"spearman\"), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Transformer Health Metrics')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_correlated_features(data, k, correlation_type='pearson'):\n    \"\"\"\n    Find the k features that are best correlated with all other features in the dataset.\n\n    Parameters:\n    - data (pd.DataFrame): The dataset containing the features.\n    - k (int): The number of top features to select.\n    - correlation_type (str): The type of correlation to compute ('pearson', 'spearman', 'kendall').\n\n    Returns:\n    - list: The names of the top k correlated features.\n    \"\"\"\n    if correlation_type not in ['pearson', 'spearman', 'kendall']:\n        raise ValueError(\"Invalid correlation_type. Choose from 'pearson', 'spearman', or 'kendall'.\")\n\n    # Compute the correlation matrix\n    corr_matrix = data.corr(method=correlation_type)\n\n    # Calculate the sum of the absolute correlations for each feature (excluding self-correlation)\n    absolute_corr_sum = corr_matrix.abs().sum() - 1\n\n    # Select the top k features with the highest sum of absolute correlations\n    top_k_features = absolute_corr_sum.nlargest(k).index.tolist()\n\n    return top_k_features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_top_k_features_to_targets(data, targets, k, correlation_type='pearson'):\n    \"\"\"\n    Find the k features that are best correlated with a given set of target columns.\n\n    Parameters:\n    - data (pd.DataFrame): The dataset containing the features.\n    - targets (list of str): The names of the target columns to find correlations against.\n    - k (int): The number of top features to select.\n    - correlation_type (str): The type of correlation to compute ('pearson', 'spearman', 'kendall').\n\n    Returns:\n    - list: The names of the top k features most correlated with the target columns.\n    \"\"\"\n    if correlation_type not in ['pearson', 'spearman', 'kendall']:\n        raise ValueError(\"Invalid correlation_type. Choose from 'pearson', 'spearman', or 'kendall'.\")\n    \n    # Ensure target columns are in the dataset\n    missing_targets = [col for col in targets if col not in data.columns]\n    if missing_targets:\n        raise ValueError(f\"Target columns {missing_targets} are not present in the dataset.\")\n    \n    # Compute the correlation matrix for the whole dataset\n    corr_matrix = data.corr(method=correlation_type)\n    \n    # Filter the correlation matrix for only the target columns\n    target_corr = corr_matrix[targets]\n    \n    # Sum the absolute correlations for each feature across all target columns\n    absolute_corr_sum = target_corr.abs().sum(axis=1)\n    \n    # Exclude the target columns themselves from being selected\n    absolute_corr_sum = absolute_corr_sum.drop(targets, errors='ignore')\n    \n    # Select the top k features with the highest sum of absolute correlations\n    top_k_features = absolute_corr_sum.nlargest(k).index.tolist()\n    \n    return top_k_features\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"targets = ['Life expectation', 'Health index']\ntop_correlated = find_top_k_features_to_targets(hlth_df, targets, 7)\ntop_correlated","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_most_correlated_grid(df, top_correlated_columns):\n    \"\"\"\n    Plot regression plots for the most correlated columns, where each column is plotted\n    against all other columns in the provided list (excluding itself).\n\n    Parameters:\n    - df (pd.DataFrame): The dataset containing the features.\n    - top_correlated_columns (list of str): A list of feature names representing the most correlated columns.\n    \"\"\"\n    # Generate all column pairs (each column with every other column excluding itself)\n    column_pairs = [(col1, col2) for i, col1 in enumerate(top_correlated_columns) \n                    for j, col2 in enumerate(top_correlated_columns) if i != j]\n\n    # Deduplicate pairs (order doesn't matter)\n    unique_pairs = []\n    seen = set()\n    for col1, col2 in column_pairs:\n        if (col1, col2) not in seen and (col2, col1) not in seen:\n            unique_pairs.append((col1, col2))\n            seen.add((col1, col2))\n\n    # Determine the grid size\n    n = len(unique_pairs)\n    grid_size = int(math.ceil(math.sqrt(n)))\n\n    # Plotting\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(5 * grid_size, 5 * grid_size))\n    axes = axes.flatten()  # Flatten the axes array for easy indexing\n\n    for ax, (feature1, feature2) in zip(axes, unique_pairs):\n        sns.regplot(x=feature1, y=feature2, data=df, ax=ax)\n        ax.set_title(f'{feature1} vs {feature2}')\n        ax.grid()\n\n    # Remove unused subplots if n doesn't exactly fill grid\n    for i in range(len(unique_pairs), len(axes)):\n        fig.delaxes(axes[i])\n    plt.tight_layout()\n    plt.show()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_most_correlated_grid(hlth_df, top_correlated + targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"column_pairs = [(col1, col2) for i, col1 in enumerate(top_correlated) \n                    for j, col2 in enumerate(top_correlated) if i != j]\n# Use a set to ensure unique pairs, ignoring order\ncolumn_pairs = set(tuple(sorted((col1, col2))) for col1, col2 in column_pairs)\n\n# Convert the set back to a list if needed\ncolumn_pairs = list(column_pairs)\n\ncolumn_pairs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhancend_hlth_df = hlth_data.copy()\nfor (feat1, feat2) in column_pairs:\n    enhancend_hlth_df[feat1 + '_' + feat2 + '_pondavg'] = np.sqrt(hlth_df[feat1] * hlth_df[feat2])\n    if feat1 in enhancend_hlth_df.columns:\n        enhancend_hlth_df.drop(columns=feat1, inplace=True)\n    elif feat2 in enhancend_hlth_df.columns:\n        enhancend_hlth_df.drop(columns=feat2, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhancend_hlth_df.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhancend_hlth_df.tail()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enhancend_hlth_df.replace([float('inf'), np.nan], 0.0, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(24, 12))\nsns.heatmap(enhancend_hlth_df.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Transformer Health Metrics')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"new_top_correlated = find_top_k_features_to_targets(enhancend_hlth_df, targets, 7)\nnew_top_correlated","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_most_correlated_grid(new_df, new_top_correlated + targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_df = new_df[find_top_k_features_to_targets(enhancend_hlth_df, targets, 12)].copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_df.tail()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from scipy import stats\n\nfor col in input_df:\n    z_scores = stats.zscore(input_df[col])\n    outliers = input_df[(z_scores > 3) | (z_scores < -3)]\n    print(f\"Outliers in {col}: {len(outliers)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"outlier_idxs = []\nfor col in input_df:\n    z_scores = stats.zscore(input_df[col])\n    outlier_idxs.extend(list(input_df[(z_scores > 3) | (z_scores < -3)].index))\noutlier_idxs = list(set(outlier_idxs))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_df.drop(outlier_idxs, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nsns.heatmap(pd.concat([input_df, new_df.loc[input_df.index][targets]], axis=1).corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Heatmap of Transformer Health Metrics')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_most_correlated_grid(pd.concat([input_df, new_df.loc[input_df.index][targets]], axis=1), new_top_correlated + targets)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_df.to_numpy().shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Features and target\nX = input_df.to_numpy()\ny = new_df.loc[input_df.index][targets[1]].to_numpy().reshape((-1, 1)) #Health Index\n\n# Splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\n# Initialize and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport numpy as np\n\n# Predictions\ny_pred = model.predict(X_test)\n\n# Evaluation metrics\nmae = mean_absolute_error(y_test, y_pred)\nrmse = np.sqrt(mean_squared_error(y_test, y_pred))\n\nprint(f\"Mean Absolute Error: {mae}\")\nprint(f\"Root Mean Squared Error: {rmse}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.predict(X_test[[0]]), y_test[[0]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\n# Initialize and train a Random Forest model\nrf_model = RandomForestRegressor(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train.ravel())\n\n# Evaluate the Random Forest model\nrf_pred = rf_model.predict(X_test)\nrf_mae = mean_absolute_error(y_test, rf_pred)\nrf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))\n\nprint(f\"Random Forest MAE: {rf_mae}\")\nprint(f\"Random Forest RMSE: {rf_rmse}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_model.predict(X_test[[0]]), y_test[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_scaled = StandardScaler().fit_transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import KBinsDiscretizer, StandardScaler, OrdinalEncoder","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discretizer = KBinsDiscretizer(n_bins=5, encode=\"ordinal\", strategy = 'uniform', random_state=42)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Features and target\ny_discretized = discretizer.fit_transform(new_df.loc[input_df.index][targets[0]].to_numpy().reshape((-1, 1))).ravel()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"discretizer.bin_edges_","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit \nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the data\ntrain_set_split, test_set_split = next(split.split(X_scaled, y_discretized))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\n# Initialize and train the model\nclf = LogisticRegression(max_iter=1000, random_state=42)\nclf.fit(X_train, y_train.ravel())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classes = ['Very Short', 'Short', 'Moderate', 'Long', 'Very Long']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_discretized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T15:46:36.653859Z","iopub.execute_input":"2025-01-19T15:46:36.654280Z","iopub.status.idle":"2025-01-19T15:46:36.663827Z","shell.execute_reply.started":"2025-01-19T15:46:36.654250Z","shell.execute_reply":"2025-01-19T15:46:36.662568Z"}},"outputs":[{"execution_count":204,"output_type":"execute_result","data":{"text/plain":"array([0., 1., 0., 0., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n       1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 2., 0.,\n       2., 0., 0., 0., 0., 1., 1., 1., 1., 1., 2., 0., 0., 0., 2., 2., 0.,\n       2., 1., 2., 2., 2., 2., 2., 2., 1., 1., 1., 1., 2., 1., 2., 1., 2.,\n       2., 1., 2., 2., 0., 1., 2., 2., 2., 2., 0., 0., 1., 2., 1., 2., 2.,\n       2., 1., 1., 0., 0., 0., 1., 1., 2., 1., 1., 0., 1., 2., 2., 2., 1.,\n       2., 1., 2., 2., 1., 0., 1., 2., 2., 1., 0., 0., 2., 2., 2., 0., 1.,\n       2., 0., 4., 0., 4., 1., 4., 1., 4., 4., 0., 4., 4., 4., 3., 4., 4.,\n       4., 4., 4., 4., 4., 0., 0., 4., 4., 4., 4., 4., 4., 4., 4., 1., 4.,\n       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n       1., 4., 4., 4., 4., 4., 4., 4., 3., 4., 4., 4., 4., 4., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 2., 4., 4., 4., 0., 0., 4., 4., 4., 4.,\n       0., 4., 4., 4., 4., 4., 4., 4., 4., 0., 4., 1., 4., 1., 1., 1., 4.,\n       4., 4., 3., 4., 4., 4., 4., 4., 0., 4., 4., 0., 4., 4., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 0., 4., 4.,\n       4., 0., 4., 1., 4., 4., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 1., 1., 4., 3., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 0., 1., 4., 4., 1., 4., 4., 4., 4., 4.,\n       4., 4., 4., 4., 4., 4., 4., 1., 4., 1., 1., 1., 0., 1., 0., 1., 0.,\n       4., 4., 0., 0., 0., 0., 0., 0., 3., 1., 1., 1., 1., 2., 3., 3., 4.,\n       4., 1., 1., 1., 1., 0., 1., 1., 0., 1., 4., 4., 1., 1., 1., 1., 1.,\n       4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 1., 1., 4., 4., 4.,\n       4., 4., 4., 4., 1., 4., 4., 0., 4., 4.])"},"metadata":{}}],"execution_count":204},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Predictions\ny_pred = clf.predict(X_test)\n\n# Evaluation metrics\nprint(classification_report(y_test, y_pred, target_names=classes, zero_division=1))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}